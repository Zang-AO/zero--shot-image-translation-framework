# ğŸ“š ZSXT Project Overview

Complete reference guide for project structure and organization.

---

## ğŸ“ Project Structure

```
_code_EN/
â”œâ”€â”€ ğŸ“‹ Documentation
â”‚   â”œâ”€â”€ README.md                    â† START: Main guide
â”‚   â”œâ”€â”€ QUICKSTART.md               â† 5-minute setup
â”‚   â”œâ”€â”€ ENVIRONMENT_SETUP.md        â† Detailed installation
â”‚   â””â”€â”€ PROJECT_OVERVIEW.md         â† This file
â”‚
â”œâ”€â”€ âš™ï¸ Configuration & Tools
â”‚   â”œâ”€â”€ config.yaml                 â† Hyperparameters
â”‚   â”œâ”€â”€ requirements.txt            â† Dependencies
â”‚   â””â”€â”€ verify_env.py               â† Verification script
â”‚
â”œâ”€â”€ ğŸš€ Scripts
â”‚   â”œâ”€â”€ train.py                    â† Training (source domain)
â”‚   â””â”€â”€ inference.py                â† Zero-shot inference
â”‚
â”œâ”€â”€ ğŸ”§ Core Modules (src/)
â”‚   â”œâ”€â”€ model.py                    â† Networks (UNet + PatchGAN)
â”‚   â”œâ”€â”€ losses.py                   â† Loss functions (dynamic)
â”‚   â”œâ”€â”€ preprocess_pipeline.py      â† Data processing
â”‚   â”œâ”€â”€ super_resolution.py         â† SR post-processing
â”‚   â””â”€â”€ img_process_train.py        â† Augmentation utils
â”‚
â”œâ”€â”€ ğŸ“Š Data (datasets/)
â”‚   â”œâ”€â”€ Source_domain/KDXray/       â† Training images (RGB)
â”‚   â””â”€â”€ Target_domain/CLC/          â† Inference images
â”‚
â””â”€â”€ ğŸ’¾ Outputs
    â”œâ”€â”€ checkpoints/                â† Model weights
    â””â”€â”€ generated_images/           â† Visualizations
```

---

## ğŸš€ Quick Navigation

### ğŸ‘¤ For New Users

| Goal | Read | Time |
|------|------|------|
| **Get started now** | [QUICKSTART.md](QUICKSTART.md) | 5 min |
| **Setup environment** | [ENVIRONMENT_SETUP.md](ENVIRONMENT_SETUP.md) | 15 min |
| **Learn everything** | [README.md](README.md) | 30 min |
| **Understand architecture** | [PROJECT_OVERVIEW.md](#architecture-guide) | 10 min |

### ğŸ‘¨â€ğŸ’» For Developers

| Topic | File | Lines |
|-------|------|-------|
| Generator & Discriminator | `src/model.py` | 85-150 |
| Loss Function Design | `src/losses.py` | 25-120 |
| Data Processing | `src/preprocess_pipeline.py` | All |
| Augmentation Strategy | `src/img_process_train.py` | 1-80 |

---

## ğŸ”§ Configuration Files

### config.yaml

**Purpose**: Single source of truth for all hyperparameters

**Key Sections**:
```yaml
# Training
batch_size: 3
num_epochs: 50
learning_rate: 0.0002

# Data paths

---

## âš™ï¸ Configuration Reference

### config.yaml Structure

```yaml
# ğŸ“Š Dataset Configuration
dataset:
  source: 'KDXray'                # Source domain
  data_root: "datasets/Source_domain/KDXray"
  images_folder: "train/images"
  batch_size: 32

# ğŸ¯ Loss Weights (3-stage dynamic)
loss_weights:
  l1: [0.5, 5.0, 50]              # Early â†’ Late epochs
  gan: [1.0, 1.0, 0.5]            # Adversarial balance
  perceptual: [3.0, 2.0, 1.0]     # Feature similarity
  color: [20.0, 30.0, 30.0]       # Color preservation

# ğŸ“ˆ Training Hyperparameters
train:
  epochs: 50
  learning_rate: 0.0002
  batch_size: 32
  num_workers: 4

# ğŸ–¼ï¸ Image Settings
image:
  img_width: 512
  img_height: 512
```

### Key Settings by Use Case

| Scenario | Setting | Value | Notes |
|----------|---------|-------|-------|
| **Fast training** | batch_size | 64 | Requires 40GB+ VRAM |
| **Limited VRAM** | batch_size | 2 | ~6GB VRAM required |
| **High resolution** | img_width/height | 640 | Slower but better quality |
| **Quick test** | epochs | 5 | For testing only |

---

## ğŸš€ Scripts Guide

### ğŸ“ train.py - Training Script

**Function**: Train ZSXT on source domain

**Key Features**:
- âœ… Automatic grayscale generation
- âœ… Dynamic 3-stage loss scheduling  
- âœ… Real-time metric evaluation
- âœ… Best model checkpoint selection

**Quick Start**:
```bash
python train.py                    # Uses config.yaml
```

**Example: Resume Training**:
```yaml
# In config.yaml
pretrained_gen: "checkpoints/gen_epoch_20.pth"
```

**Outputs**:
```
checkpoints/
â”œâ”€â”€ gen_best.pth           â† Best overall model
â”œâ”€â”€ gen_best_mae.pth       â† Best pixel accuracy
â””â”€â”€ gen_epoch_N.pth        â† Periodic checkpoints

generated_images/
â”œâ”€â”€ epoch_10_samples.png   â† Visual progression
â”œâ”€â”€ training_curves.png    â† Loss curves
â””â”€â”€ evaluation_metrics.png â† Metric plots
```

---

### ğŸ¯ inference.py - Zero-Shot Inference

**Function**: Translate target domain images (no retraining)

**Command**:
```bash
python inference.py \
  --input path/to/images \
  --output path/to/output \
  --checkpoint checkpoints/gen_best.pth
```

| Parameter | Purpose | Example |
|-----------|---------|---------|
| `--input` | Source images | `datasets/Target/images` |
| `--output` | Output folder | `datasets/Output/images` |
| `--checkpoint` | Model path | `checkpoints/gen_best.pth` |

**Output Example**:
```
âœ… Processing 542 images
Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 542/542 [00:23]
âœ… Completed! Results saved.
```

---

### ğŸ” verify_env.py - Environment Check

**Function**: Validate all dependencies and GPU setup

**Command**:
```bash
python verify_env.py
```

**Expected Output**:
```
âœ“ Python: 3.9.18
âœ“ PyTorch: 2.1.0
âœ“ CUDA: Available
âœ“ GPU: NVIDIA RTX 3090 (24GB)
âœ“ All dependencies: OK
âœ… Ready for training!
```

---

## ğŸ§© Core Modules Overview

### src/model.py - Neural Networks

**Components**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GeneratorUNet                     â”‚
â”‚   (8-layer, 34.9M parameters)      â”‚
â”‚   Gray (1-ch) â†’ RGB (3-ch)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
        Real Domain RGB
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PatchGANDiscriminator             â”‚
â”‚   (5-layer, 2.77M parameters)       â”‚
â”‚   70Ã—70 receptive field             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Architecture Table**:
| Component | Type | Params | Input | Output |
|-----------|------|--------|-------|--------|
| Generator | UNet | 34.9M | [B,1,H,W] | [B,3,H,W] |
| Discriminator | PatchGAN | 2.77M | [B,4,H,W] | [B,1,70,70] |
| **Total** | - | **37.7M** | - | - |

**Key Usage**:
```python
generator = GeneratorUNet(in_ch=1, out_ch=3)
discriminator = PatchGANDiscriminator(in_ch=4)

fake_rgb = generator(gray_image)           # [B,1,H,W]â†’[B,3,H,W]
disc_pred = discriminator(cat([gray, rgb])) # Judge authenticity
```

---

### src/losses.py - Loss Functions

**Four-Component Design**:

| Component | Weight | Purpose | Formula |
|-----------|--------|---------|---------|
| **L1 Loss** | 70% | Pixel accuracy | âˆ‘\|fake - real\| |
| **GAN Loss** | 10% | Adversarial training | BCE |
| **Perceptual** | 15% | Feature matching | VGG19 distance |
| **Color Loss** | 5% | Histogram alignment | KL divergence |

**Dynamic Scheduling** (3 Stages):

```
Epoch Progress: 0% â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ 100%
                    Stage 1 â”‚ Stage 2  â”‚ Stage 3

L1 Weight:        0.5 â”€â”€â”€â”€â”€â†’ 5.0 â”€â”€â”€â”€â†’ 50 (focuses on detail)
GAN Weight:       1.0 â”€â”€â”€â”€â”€â†’ 1.0 â”€â”€â”€â”€â†’ 0.5 (reduces collapse)
Perceptual:       3.0 â”€â”€â”€â”€â”€â†’ 2.0 â”€â”€â”€â”€â†’ 1.0
Color Loss:      20.0 â”€â”€â”€â”€â†’ 30.0 â”€â”€â”€â”€â†’ 30.0 (constant preservation)
```

**Usage Example**:
```python
loss_fn = CombinedLoss(total_epochs=50, weights=config.loss_weights)
loss_fn.set_epoch(15)  # Sets weights based on epoch

loss_g, detail_dict = loss_fn.forward_generator(
    fake=gen_output,
    real=real_image,
    disc_output=disc_pred
)
```

---

### src/preprocess_pipeline.py - Data Processing

**Pipeline Stages**:

```
RGB Image (640Ã—640)
      â†“ [1] Super-Resolution (2Ã—)
RGB Image (1280Ã—1280)
      â†“ [2] Resize to target
RGB Image (512Ã—512)
      â†“ [3] Decolorization (ITU-R BT.601)
Gray Image (512Ã—512)
      â†“ [4] Multi-modal Augmentation (Ã—3)
Augmented Gray Images (3 variants each)
```

**Augmentation Strategy**:

| Augmentation | Parameters | Purpose |
|--------------|-----------|---------|
| Poisson Noise | Ïƒ=0.05 | Quantum noise simulation |
| Brightness | 0.8-1.2Ã— | Illumination variation |
| Ripple | 5px amplitude | Motion artifacts |
| Metal Artifacts | 0.5 intensity | Equipment artifacts |
| Lens Flare | 0.5 intensity | Optical artifacts |

**Code Example**:
```python
pipeline = PreprocessPipeline('config.yaml')
pipeline.check_and_generate(
    images_folder='datasets/train/images',
    gray_folder='datasets/train/images_gray'
)
# Generates 3 augmented versions per image
```

---

### src/super_resolution.py

**Methods**:
- **Bicubic interpolation** (GPU-accelerated, default)
- **Deep SR model** (optional, ESPCN-like)

**Usage**:
```python
sr = SuperResolution(scale_factor=2, device='cuda')

# NumPy interface
upsampled = sr.upsample_numpy(image_np)  # [H, W, 3] â†’ [2H, 2W, 3]

# Tensor interface
upsampled_tensor = sr.upsample(image_tensor)  # [B, 3, H, W] â†’ [B, 3, 2H, 2W]
```

---

## ğŸ“Š Expected Outputs

### Training Outputs

**Checkpoints** (`checkpoints/`):
```
gen_best.pth           # Best overall loss (recommended for general use)
gen_best_mae.pth       # Best MAE (recommended for pixel accuracy)
gen_final.pth          # Final epoch
gen_epoch_10.pth       # Periodic checkpoint (every save_interval)
```

**Visualizations** (`generated_images/`):
```
epoch_50_samples.png              # Input | Generated | Ground Truth
training_curves.png               # Loss curves (G, D, L1, GAN, Perc, Color)
evaluation_metrics_curves.png     # Metric curves (MAE, FID, LPIPS, Color-KL)
```

### Inference Outputs

**Translated Images**:
```
datasets/Target_domain/CLC_extract_ZSXT/
â”œâ”€â”€ images/                  # Translated images (same resolution as config)
â”‚   â”œâ”€â”€ image001.png
â”‚   â”œâ”€â”€ image002.png
â”‚   â””â”€â”€ ...
â””â”€â”€ labels/                  # Copied from input (if --no-copy-labels not set)
    â”œâ”€â”€ image001.txt
    â””â”€â”€ ...
```

---

## ğŸ“ˆ Performance Benchmarks

### Training Speed (RTX 3090, batch_size=3, 640Ã—640)

| Operation | Time | Throughput |
|-----------|------|------------|
| Single epoch | ~3 min | ~30 img/s |
| Full training (50 epochs) | ~2.5 hours | - |
| Preprocessing | ~10 min (9k images) | ~15 img/s |

### Inference Speed

| GPU | Resolution | Time/Image | Throughput |
|-----|------------|------------|------------|
| RTX 3090 | 640Ã—640 | 42ms | 24 fps |
| RTX 3090 | 960Ã—960 | 89ms | 11 fps |
| RTX 3060 | 640Ã—640 | 68ms | 15 fps |
| CPU (i7-9700K) | 640Ã—640 | 2.3s | 0.4 fps |

### GPU Memory Usage

| Batch Size | Resolution | Memory (Training) | Memory (Inference) |
|------------|------------|-------------------|---------------------|
| 3 | 640Ã—640 | ~8GB | ~2GB |
| 8 | 640Ã—640 | ~18GB | - |
| 16 | 640Ã—640 | ~32GB (A100) | - |
| 1 | 960Ã—960 | ~4GB | ~3GB |

---

## ğŸ” Troubleshooting Quick Reference

### Training Issues

| Issue | Solution |
|-------|----------|
| OOM error | Reduce `batch_size` in config.yaml |
| Discriminator collapse | Increase `loss_weights.gan` |
| Slow training | Enable `torch.backends.cudnn.benchmark = True` |
| Poor visual quality | Increase `loss_weights.perceptual` |

### Inference Issues

| Issue | Solution |
|-------|----------|
| Output too dark/bright | Check input images are RGB (not BGR) |
| Blurry results | Ensure using `gen_best_mae.pth` checkpoint |
| Slow inference | Reduce `inference.img_width/height` |
| Labels not copied | Remove `--no-copy-labels` flag |

### Environment Issues

| Issue | Solution |
|-------|----------|
| CUDA not found | Reinstall PyTorch with CUDA |
| Import cv2 fails | `pip install opencv-python==4.8.0` |
| VGG19 download fails | Pre-download: `python -c "import torchvision; torchvision.models.vgg19(weights='VGG19_Weights.IMAGENET1K_V1')"` |

---

## ğŸ“š Additional Resources

### Paper & Dataset
- **Paper**: [Coming Soon - IEEE Conference 2025]
- **Dataset**: [PDSXray on Figshare](https://figshare.com/s/70c31a8d9c7d0f0f8fc5)

### Code References
- **PyTorch**: https://pytorch.org/
- **OpenCV**: https://opencv.org/
- **Baseline Methods**: CycleGAN, CUT, UVCGAN, EnCo

### Community
- **GitHub Issues**: [Report bugs](https://github.com/Zang-AO/zero--shot-image-translation-framework/issues)
- **Email**: syx2821@cau.edu.cn (Corresponding Author)

---

## âœ… Checklist for New Users

### Before Training
- [ ] Environment verified (`python verify_env.py`)
- [ ] Dataset placed in `datasets/Source_domain/.../train/images/`
- [ ] Config reviewed (`config.yaml`)
- [ ] GPU available (`nvidia-smi`)

### During Training
- [ ] Monitor console logs (Loss_G, Loss_D, Gap)
- [ ] Check sample images (`generated_images/epoch_N_samples.png`)
- [ ] Track metrics (MAEâ†“, FIDâ†“, LPIPSâ†“, Color-KLâ†“)

### After Training
- [ ] Best checkpoint saved (`checkpoints/gen_best_mae.pth`)
- [ ] Final metrics: MAE<0.03, FID<20
- [ ] Inference tested on target domain
- [ ] Detection accuracy evaluated (optional)

---

**Version**: 1.0.0  
**Last Updated**: 2025-01-XX  
**Maintainer**: Xiaohao Zhang (Corresponding: Yinxue Shi)
